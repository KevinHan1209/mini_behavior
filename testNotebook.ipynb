{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_minigrid.wrappers import ImgObsWrapper\n",
    "from mini_behavior.utils.wrappers import MiniBHFullyObsWrapper\n",
    "from mini_behavior.register import register\n",
    "import mini_behavior\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "TASK = 'PlayAlligator'\n",
    "PARTIAL_OBS = True\n",
    "ROOM_SIZE = 10\n",
    "MAX_STEPS = 1000\n",
    "TOTAL_TIMESTEPS = 5e6\n",
    "DENSE_REWARD = False\n",
    "POLICY_TYPE = 'CnnPolicy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 512, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register env PlayAlligator\n",
      "atsamelocation\n",
      "infovofrobot\n",
      "True\n",
      "inhandofrobot\n",
      "False\n",
      "inreachofrobot\n",
      "False\n",
      "insameroomasrobot\n",
      "True\n",
      "inside\n",
      "nextto\n",
      "onfloor\n",
      "True\n",
      "onTop\n",
      "under\n",
      "atsamelocation\n",
      "infovofrobot\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in_view() argument after * must be an iterable, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     17\u001b[0m register(\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39menv_name,\n\u001b[1;32m     19\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_behavior.envs:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTASK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mEnv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     24\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: POLICY_TYPE,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m: TOTAL_TIMESTEPS,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: env_name,\n\u001b[1;32m     28\u001b[0m }\n\u001b[0;32m---> 30\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PARTIAL_OBS:\n\u001b[1;32m     32\u001b[0m     env \u001b[38;5;241m=\u001b[39m MiniBHFullyObsWrapper(env)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/babyRL/lib/python3.8/site-packages/gym/envs/registration.py:235\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/babyRL/lib/python3.8/site-packages/gym/envs/registration.py:129\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking new env: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m    128\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec(path)\n\u001b[0;32m--> 129\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/babyRL/lib/python3.8/site-packages/gym/envs/registration.py:90\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[0;32m---> 90\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m spec \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/mini_behavior/mini_behavior/envs/play_alligator.py:35\u001b[0m, in \u001b[0;36mPlayAlligatorEnv.__init__\u001b[0;34m(self, mode, room_size, num_rows, num_cols, max_steps)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj1\u001b[38;5;241m.\u001b[39mstates[state_value], RelativeObjectState):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mobj1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate_value\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/mini_behavior/mini_behavior/states.py:18\u001b[0m, in \u001b[0;36mInFOVOfRobot.get_value\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, env):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_view\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcur_pos\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: in_view() argument after * must be an iterable, not NoneType"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MinigridFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "\n",
    "# Env wrapping\n",
    "env_name = f\"MiniGrid-{TASK}-{ROOM_SIZE}x{ROOM_SIZE}-N2-v0\"\n",
    "\n",
    "print(f'register env {TASK}')\n",
    "\n",
    "kwargs = {\"room_size\": ROOM_SIZE, \"max_steps\": MAX_STEPS}\n",
    "\n",
    "if DENSE_REWARD:\n",
    "    assert TASK in [\"PuttingAwayDishesAfterCleaning\", \"WashingPotsAndPans\"]\n",
    "    kwargs[\"dense_reward\"] = True\n",
    "\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=f'mini_behavior.envs:{TASK}Env',\n",
    "    kwargs=kwargs\n",
    ")\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy_type\": POLICY_TYPE,\n",
    "    \"total_timesteps\": TOTAL_TIMESTEPS,\n",
    "    \"env_name\": env_name,\n",
    "}\n",
    "\n",
    "env = gym.make(env_name)\n",
    "if not PARTIAL_OBS:\n",
    "    env = MiniBHFullyObsWrapper(env)\n",
    "env = ImgObsWrapper(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "print('begin training')\n",
    "# Policy training\n",
    "model = PPO(config[\"policy_type\"], env, n_steps=8000, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(config[\"total_timesteps\"], callback=StopTrainingOnMaxEpisodes(max_episodes=3000))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not PARTIAL_OBS:\n",
    "    model.save(f\"models/ppo_cnn/{env_name}\")\n",
    "else:\n",
    "    model.save(f\"models/ppo_cnn_partial/{env_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m1', 'm2']\n",
      "['m1', 'm3']\n",
      "['t1']\n",
      "['t1']\n"
     ]
    }
   ],
   "source": [
    "test = {'mallet': ['m1', 'm2'], 'toy': ['t1']}\n",
    "test2 = {'mallet': ['m1', 'm3'], 'toy': ['t1']}\n",
    "def objdiff(objs1, objs2):\n",
    "    for obj1, obj2 in zip(objs1.values(), objs2.values()):\n",
    "        print(obj1)\n",
    "        print(obj2)\n",
    "objdiff(test, test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
