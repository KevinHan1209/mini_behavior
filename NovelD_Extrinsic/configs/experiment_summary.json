{
  "total_experiments": 2,
  "base_hyperparameters": {
    "alpha": 0.5,
    "update_proportion": 1,
    "int_gamma": 0.99,
    "ent_coef": 0.01,
    "learning_rate": 0.0003,
    "num_envs": 8,
    "num_steps": 125,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "num_minibatches": 4,
    "update_epochs": 4,
    "clip_coef": 0.2,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "int_coef": 0.0,
    "ext_coef": 1.0,
    "total_timesteps": 2500000
  },
  "reward_categories": [
    "noise",
    "interaction",
    "location_change"
  ],
  "experiments": [
    {
      "name": "exp_000_pure_intrinsic_baseline",
      "description": "Pure intrinsic rewards baseline (NovelD only)",
      "extrinsic_rewards": {},
      "int_coef": 1.0,
      "ext_coef": 0.0
    },
    {
      "name": "exp_001_pure_extrinsic_all_rewards",
      "description": "Pure extrinsic rewards only (no intrinsic motivation)",
      "extrinsic_rewards": {
        "noise": 0.1,
        "interaction": 0.1,
        "location_change": 0.1
      },
      "int_coef": 0.0,
      "ext_coef": 1.0
    }
  ]
}