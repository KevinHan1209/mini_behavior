{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "from networks.actor import ActorNetwork\n",
    "from networks.critic import Critic\n",
    "from numpy import linalg as LA\n",
    "from mini_behavior.roomgrid import *\n",
    "from mini_behavior.utils.utils import dir_to_rad, schedule\n",
    "import random\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gym.wrappers.normalize import RunningMeanStd\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordEpisodeStatistics(gym.Wrapper):\n",
    "    def __init__(self, env, deque_size=100):\n",
    "        super().__init__(env)\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.episode_returns = None\n",
    "        self.episode_lengths = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observations = super().reset(**kwargs)\n",
    "        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        return observations\n",
    "\n",
    "    def step(self, action):\n",
    "        observations, rewards, dones, infos = super().step(action)\n",
    "        self.episode_returns += infos[\"reward\"]\n",
    "        self.episode_lengths += 1\n",
    "        self.returned_episode_returns[:] = self.episode_returns\n",
    "        self.returned_episode_lengths[:] = self.episode_lengths\n",
    "        self.episode_returns *= 1 - infos[\"terminated\"]\n",
    "        self.episode_lengths *= 1 - infos[\"terminated\"]\n",
    "        infos[\"r\"] = self.returned_episode_returns\n",
    "        infos[\"l\"] = self.returned_episode_lengths\n",
    "        return (\n",
    "            observations,\n",
    "            rewards,\n",
    "            dones,\n",
    "            infos,\n",
    "        )\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 7 * 7, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 448)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.extra_layer = nn.Sequential(layer_init(nn.Linear(448, 448), std=0.1), nn.ReLU())\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(448, 448), std=0.01),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(448, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "        self.critic_ext = layer_init(nn.Linear(448, 1), std=0.01)\n",
    "        self.critic_int = layer_init(nn.Linear(448, 1), std=0.01)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        features = self.extra_layer(hidden)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return (\n",
    "            action,\n",
    "            probs.log_prob(action),\n",
    "            probs.entropy(),\n",
    "            self.critic_ext(features + hidden),\n",
    "            self.critic_int(features + hidden),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        features = self.extra_layer(hidden)\n",
    "        return self.critic_ext(features + hidden), self.critic_int(features + hidden)\n",
    "\n",
    "class RewardForwardFilter:\n",
    "    def __init__(self, gamma):\n",
    "        self.rewems = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.rewems is None:\n",
    "            self.rewems = rews\n",
    "        else:\n",
    "            self.rewems = self.rewems * self.gamma + rews\n",
    "        return self.rewems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 128\n",
    "num_steps = 128\n",
    "num_minibatches = 4\n",
    "total_timesteps = 2000000000\n",
    "\n",
    "\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "num_iterations = total_timesteps // batch_size\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = envpool.make(\n",
    "    args.env_id,\n",
    "    env_type=\"gym\",\n",
    "    num_envs=args.num_envs,\n",
    "    episodic_life=True,\n",
    "    reward_clip=True,\n",
    "    seed=args.seed,\n",
    "    repeat_action_probability=0.25,\n",
    ")\n",
    "envs.num_envs = args.num_envs\n",
    "envs.single_action_space = envs.action_space\n",
    "envs.single_observation_space = envs.observation_space\n",
    "envs = RecordEpisodeStatistics(envs)\n",
    "assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "rnd_model = RNDModel(4, envs.single_action_space.n).to(device)\n",
    "combined_parameters = list(agent.parameters()) + list(rnd_model.predictor.parameters())\n",
    "optimizer = optim.Adam(\n",
    "    combined_parameters,\n",
    "    lr=args.learning_rate,\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "reward_rms = RunningMeanStd()\n",
    "obs_rms = RunningMeanStd(shape=(1, 1, 84, 84))\n",
    "discounted_reward = RewardForwardFilter(args.int_gamma)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "curiosity_rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "ext_values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "int_values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "avg_returns = deque(maxlen=20)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)\n",
    "num_updates = args.total_timesteps // args.batch_size\n",
    "\n",
    "print(\"Start to initialize observation normalization parameter.....\")\n",
    "next_ob = []\n",
    "for step in range(args.num_steps * args.num_iterations_obs_norm_init):\n",
    "    acs = np.random.randint(0, envs.single_action_space.n, size=(args.num_envs,))\n",
    "    s, r, d, _ = envs.step(acs)\n",
    "    next_ob += s[:, 3, :, :].reshape([-1, 1, 84, 84]).tolist()\n",
    "\n",
    "    if len(next_ob) % (args.num_steps * args.num_envs) == 0:\n",
    "        next_ob = np.stack(next_ob)\n",
    "        obs_rms.update(next_ob)\n",
    "        next_ob = []\n",
    "print(\"End to initialize...\")\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if args.anneal_lr:\n",
    "        frac = 1.0 - (update - 1.0) / num_updates\n",
    "        lrnow = frac * args.learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += 1 * args.num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            value_ext, value_int = agent.get_value(obs[step])\n",
    "            ext_values[step], int_values[step] = (\n",
    "                value_ext.flatten(),\n",
    "                value_int.flatten(),\n",
    "            )\n",
    "            action, logprob, _, _, _ = agent.get_action_and_value(obs[step])\n",
    "\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        rnd_next_obs = (\n",
    "            (\n",
    "                (next_obs[:, 3, :, :].reshape(args.num_envs, 1, 84, 84) - torch.from_numpy(obs_rms.mean).to(device))\n",
    "                / torch.sqrt(torch.from_numpy(obs_rms.var).to(device))\n",
    "            ).clip(-5, 5)\n",
    "        ).float()\n",
    "        target_next_feature = rnd_model.target(rnd_next_obs)\n",
    "        predict_next_feature = rnd_model.predictor(rnd_next_obs)\n",
    "        curiosity_rewards[step] = ((target_next_feature - predict_next_feature).pow(2).sum(1) / 2).data\n",
    "        for idx, d in enumerate(done):\n",
    "            if d and info[\"lives\"][idx] == 0:\n",
    "                avg_returns.append(info[\"r\"][idx])\n",
    "                epi_ret = np.average(avg_returns)\n",
    "                print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['r'][idx]}, curiosity_reward={np.mean(curiosity_rewards[step].cpu().numpy())}\"\n",
    "                )\n",
    "                writer.add_scalar(\"charts/avg_episodic_return\", epi_ret, global_step)\n",
    "                writer.add_scalar(\"charts/episodic_return\", info[\"r\"][idx], global_step)\n",
    "                writer.add_scalar(\n",
    "                    \"charts/episode_curiosity_reward\",\n",
    "                    curiosity_rewards[step][idx],\n",
    "                    global_step,\n",
    "                )\n",
    "                writer.add_scalar(\"charts/episodic_length\", info[\"l\"][idx], global_step)\n",
    "\n",
    "    curiosity_reward_per_env = np.array(\n",
    "        [discounted_reward.update(reward_per_step) for reward_per_step in curiosity_rewards.cpu().data.numpy().T]\n",
    "    )\n",
    "    mean, std, count = (\n",
    "        np.mean(curiosity_reward_per_env),\n",
    "        np.std(curiosity_reward_per_env),\n",
    "        len(curiosity_reward_per_env),\n",
    "    )\n",
    "    reward_rms.update_from_moments(mean, std**2, count)\n",
    "\n",
    "    curiosity_rewards /= np.sqrt(reward_rms.var)\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value_ext, next_value_int = agent.get_value(next_obs)\n",
    "        next_value_ext, next_value_int = next_value_ext.reshape(1, -1), next_value_int.reshape(1, -1)\n",
    "        ext_advantages = torch.zeros_like(rewards, device=device)\n",
    "        int_advantages = torch.zeros_like(curiosity_rewards, device=device)\n",
    "        ext_lastgaelam = 0\n",
    "        int_lastgaelam = 0\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                ext_nextnonterminal = 1.0 - next_done\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = next_value_ext\n",
    "                int_nextvalues = next_value_int\n",
    "            else:\n",
    "                ext_nextnonterminal = 1.0 - dones[t + 1]\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = ext_values[t + 1]\n",
    "                int_nextvalues = int_values[t + 1]\n",
    "            ext_delta = rewards[t] + args.gamma * ext_nextvalues * ext_nextnonterminal - ext_values[t]\n",
    "            int_delta = curiosity_rewards[t] + args.int_gamma * int_nextvalues * int_nextnonterminal - int_values[t]\n",
    "            ext_advantages[t] = ext_lastgaelam = (\n",
    "                ext_delta + args.gamma * args.gae_lambda * ext_nextnonterminal * ext_lastgaelam\n",
    "            )\n",
    "            int_advantages[t] = int_lastgaelam = (\n",
    "                int_delta + args.int_gamma * args.gae_lambda * int_nextnonterminal * int_lastgaelam\n",
    "            )\n",
    "        ext_returns = ext_advantages + ext_values\n",
    "        int_returns = int_advantages + int_values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_ext_advantages = ext_advantages.reshape(-1)\n",
    "    b_int_advantages = int_advantages.reshape(-1)\n",
    "    b_ext_returns = ext_returns.reshape(-1)\n",
    "    b_int_returns = int_returns.reshape(-1)\n",
    "    b_ext_values = ext_values.reshape(-1)\n",
    "\n",
    "    b_advantages = b_int_advantages * args.int_coef + b_ext_advantages * args.ext_coef\n",
    "\n",
    "    obs_rms.update(b_obs[:, 3, :, :].reshape(-1, 1, 84, 84).cpu().numpy())\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(args.batch_size)\n",
    "\n",
    "    rnd_next_obs = (\n",
    "        (\n",
    "            (b_obs[:, 3, :, :].reshape(-1, 1, 84, 84) - torch.from_numpy(obs_rms.mean).to(device))\n",
    "            / torch.sqrt(torch.from_numpy(obs_rms.var).to(device))\n",
    "        ).clip(-5, 5)\n",
    "    ).float()\n",
    "\n",
    "    clipfracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            predict_next_state_feature, target_next_state_feature = rnd_model(rnd_next_obs[mb_inds])\n",
    "            forward_loss = F.mse_loss(\n",
    "                predict_next_state_feature, target_next_state_feature.detach(), reduction=\"none\"\n",
    "            ).mean(-1)\n",
    "\n",
    "            mask = torch.rand(len(forward_loss), device=device)\n",
    "            mask = (mask < args.update_proportion).type(torch.FloatTensor).to(device)\n",
    "            forward_loss = (forward_loss * mask).sum() / torch.max(\n",
    "                mask.sum(), torch.tensor([1], device=device, dtype=torch.float32)\n",
    "            )\n",
    "            _, newlogprob, entropy, new_ext_values, new_int_values = agent.get_action_and_value(\n",
    "                b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            new_ext_values, new_int_values = new_ext_values.view(-1), new_int_values.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                ext_v_loss_unclipped = (new_ext_values - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_clipped = b_ext_values[mb_inds] + torch.clamp(\n",
    "                    new_ext_values - b_ext_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                ext_v_loss_clipped = (ext_v_clipped - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_loss_max = torch.max(ext_v_loss_unclipped, ext_v_loss_clipped)\n",
    "                ext_v_loss = 0.5 * ext_v_loss_max.mean()\n",
    "            else:\n",
    "                ext_v_loss = 0.5 * ((new_ext_values - b_ext_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            int_v_loss = 0.5 * ((new_int_values - b_int_returns[mb_inds]) ** 2).mean()\n",
    "            v_loss = ext_v_loss + int_v_loss\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef + forward_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if args.max_grad_norm:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    combined_parameters,\n",
    "                    args.max_grad_norm,\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None:\n",
    "            if approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/fwd_loss\", forward_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babyRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
